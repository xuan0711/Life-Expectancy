```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

# Instructions

**Exercises:**  1,4 (23.3.3 Exercises); 1,4 (23.4.5 Exercises)

**Submission:** Submit an electronic document on Gradescope. Must be submitted as a PDF file generated in RStudio. All assigned problems are chosen according to the textbook *R for Data Science*. You do not need R code to answer every question. If you answer without using R code, delete the code chunk. If the question requires R code, make sure you display R code. If the question requires a figure, make sure you display a figure. A lot of the questions can be answered in written response, but require R code and/or figures for understanding and explaining.

```{r, include=TRUE}
library(tidyverse)
library(modelr)
```


# Chapter 23 (23.3.3 Exercises)

##  Exercise 1
Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()?
-The predictions of loess are the same as the default method for geom_smooth() because geom_smooth() uses loess() by default
```{r}
sim1_loess <- loess(y ~ x, data = sim1)
sim1_lm <- lm(y ~ x, data = sim1)

grid_loess <- sim1 %>%
  add_predictions(sim1_loess)

sim1 <- sim1 %>%
  add_residuals(sim1_lm) %>%
  add_predictions(sim1_lm) %>%
  add_residuals(sim1_loess, var = "resid_loess") %>%
  add_predictions(sim1_loess, var = "pred_loess")
sim1
```

```{r}
plot_sim1_loess <-
 ggplot(sim1, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = pred), data = grid_loess, colour = "red")
plot_sim1_loess 
```
```{r}
plot_sim1_loess +
  geom_smooth(method = "loess", colour = "blue", se = FALSE, alpha = 0.20)
```

##  Exercise 4
Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?
-It is easier to find the distribution of the residuals with absolute values. However, the absolute value of resisuals cannot show the effect of the sign, which means that the frequency polygon cannot show whether the model systematically over- or under-estimates the residuals.

```{r}
sim1_mod <- lm(y ~ x, data = sim1)

sim1 <- sim1 %>%
  add_residuals(sim1_mod)

ggplot(sim1, aes(x = abs(resid))) +
  geom_freqpoly(binwidth = 0.5)
```

# Chapter 23 (23.4.5 Exercises)

##  Exercise 1
What happens if you repeat the analysis of sim2 using a model without an intercept. What happens to the model equation? What happens to the predictions?
-Same
```{r}
mod2a <- lm(y ~ x - 1, data = sim2)
mod2 <- lm(y ~ x, data = sim2)
```
```{r}
grid <- sim2 %>%
  data_grid(x) %>%
  spread_predictions(mod2, mod2a)
grid
```

##  Exercise 4
For sim4, which of mod1 and mod2 is better? I think mod2 does a slightly better job at removing patterns, but itâ€™s pretty subtle. Can you come up with a plot to support my claim?

```{r}
mod1 <- lm(y ~ x1 + x2, data = sim4)
mod2 <- lm(y ~ x1 * x2, data = sim4)
sim4_mods <- gather_residuals(sim4, mod1, mod2)
```
```{r}
ggplot(sim4_mods, aes(x = resid, colour = model)) +
  geom_freqpoly(binwidth = 0.5) +
  geom_rug()
```
```{r}
ggplot(sim4_mods, aes(x = abs(resid), colour = model)) +
  geom_freqpoly(binwidth = 0.5) +
  geom_rug()
```
```{r}
sim4_mods %>%
  group_by(model) %>%
  summarise(resid = sd(resid))
```

The standard deviation of the residuals of mod2 is smaller than that of mod1.